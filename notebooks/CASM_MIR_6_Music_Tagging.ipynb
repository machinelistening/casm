{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "# Computational Analysis of Sound and Music\n",
    "\n",
    "# MIR 6 - Music Tagging\n",
    "\n",
    "Dr.-Ing. Jakob Abeßer, jakob.abesser@idmt.fraunhofer.de\n",
    "\n",
    "**Last update:** 18.05.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690b683",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "In this notebook, you will learn how to implement a simple **music tagging** and **music similarity** algorithm using **deep audio embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d4aa5",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea469be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openl3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import IPython.display as ipd\n",
    "import wget\n",
    "import seaborn as sns\n",
    "import openl3\n",
    "import zipfile\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, concatenate, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eda528",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa517f8",
   "metadata": {},
   "source": [
    "We use in this notebook a small subset of the **GTZAN dataset**, which has been published in 2002 by George Tzanetakis and is one of the first genre classification datasets in the MIR field.\n",
    "You can read more about it here:\n",
    "  - https://music-classification.github.io/tutorial/part2_basics/dataset.html#gtzan-music-genre-2002\n",
    "  - George Tzanetakis and Perry Cook. Musical genre classification of audio signals. Speech and Audio Processing, IEEE transactions on, 10(5):293–302, 2002.\n",
    "  \n",
    "The **genre_mini** dataset was created for this notebook, it includes\n",
    "  - 10 music genres\n",
    "  - 10 audio clips (2.5s) each per genre (total of 100 clips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('genres_mini.zip'):\n",
    "    print('Please wait a couple of seconds ...')\n",
    "    wget.download('https://github.com/machinelistening/machinelistening.github.io/blob/master/genres_mini.zip?raw=true', \n",
    "                      out='genres_mini.zip', bar=None)\n",
    "    print('genres_mini downloaded successfully ...')\n",
    "else:\n",
    "    print('Files already exist!')\n",
    "    \n",
    "if not os.path.isdir('genres_mini.zip'):\n",
    "    print(\"Let's unzip the file ... \")\n",
    "    assert os.path.isfile('genres_mini.zip')\n",
    "    with zipfile.ZipFile('genres_mini.zip', 'r') as f:\n",
    "        # Entpacke alle Inhalte in das angegebene Verzeichnis\n",
    "        f.extractall('.')\n",
    "    assert os.path.isdir('genres_mini')\n",
    "    print(\"All done :)\")\n",
    "\n",
    "dir_dataset = 'genres_mini'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796fda1",
   "metadata": {},
   "source": [
    "Let's check our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the directory\n",
    "files = os.listdir(dir_dataset)\n",
    "\n",
    "# Print the list of files\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0a9dc",
   "metadata": {},
   "source": [
    "### Annotations\n",
    "\n",
    "Let's load the **metadata.csv** file, which includes three columns:\n",
    "- WAV file name (in the genre_mini dataset)\n",
    "- Music genre\n",
    "- Original WAV file name (from the GTZAN dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(dir_dataset, 'metadata.csv'), names=('fn_wav', 'genre', 'fn_wav_orig'))\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ffc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = sorted(list(set(df['genre'])))\n",
    "n_genres = len(unique_genres)\n",
    "genre_to_id = {unique_genres[_]: _ for _ in range(n_genres)}\n",
    "id_to_genre = {_: unique_genres[_] for _ in range(n_genres)}\n",
    "\n",
    "print(unique_genres)\n",
    "print(genre_to_id)\n",
    "print(id_to_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [genre_to_id[_] for _ in df['genre']]\n",
    "class_ids = np.array(class_ids)\n",
    "print(class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7cab3",
   "metadata": {},
   "source": [
    "Let's listen to some files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ids = [3,45,23,67,77]\n",
    "\n",
    "for i in random_ids:\n",
    "    fn_wav = os.path.join(dir_dataset, df[\"fn_wav\"][i])\n",
    "    genre = df[\"genre\"][i]\n",
    "    print(f\"File: {fn_wav} - Music Genre: {genre}\")\n",
    "    x, fs = librosa.load(fn_wav, sr=44100)\n",
    "    ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92d6d5",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a44ec",
   "metadata": {},
   "source": [
    "In this notebook, we want to compare two audio feature representations:\n",
    "\n",
    "- the **Mel-Frequency Cepstral Coefficients (MFCCs)** as an example of a traditional audio feature, which characterizes mainly the spectral envelope and therefore the timbral properties of an audio recording\n",
    "\n",
    "- the **OpenL3 deep audio embeddings** - these are extracted using a pre-trained DNN model, which has been trained in a self-supervised manner by solving an audio-video correpondance task. \n",
    "  - you can find a tutorial on how to use the **openl3** Python package here: https://openl3.readthedocs.io/en/latest/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc53fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = openl3.models.load_audio_embedding_model(input_repr=\"mel256\", \n",
    "                                                 content_type=\"music\",\n",
    "                                                 embedding_size=512)\n",
    "\n",
    "def compute_mfcc(fn_wav):\n",
    "    \"\"\" Extract time-averaged MFCC features\n",
    "    Args:\n",
    "        fn_wav (str): WAV file name\n",
    "    Returns:\n",
    "        emb (np.ndarray): 40-dimensional Mel-frequency coefficients\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(fn_wav)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    # the shape of the MFCC matrix is (40, number_of_frames), we average this over time\n",
    "    return np.mean(mfcc, axis=1)\n",
    "\n",
    "def compute_openl3(fn_wav):\n",
    "    \"\"\" Extract OpenL3 embeddings\n",
    "    Args:\n",
    "        fn_wav (str): WAV file name\n",
    "    Returns:\n",
    "        emb (np.ndarray): 512-dimensional embedding vector\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(fn_wav)\n",
    "    emb, _ = openl3.get_audio_embedding(y, sr, model=model, hop_size=0.5)\n",
    "    emb = emb.T  # transpose to shape (512, number_of_frames)\n",
    "    return np.mean(emb, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b011e2d",
   "metadata": {},
   "source": [
    "Extract time-averaged MFCC vector and OpenL3 embeddings as feature representations for each audio clip (**this takes some seconds**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = df.shape[0]\n",
    "mfcc = np.zeros((n_files, 40))\n",
    "emb = np.zeros((n_files, 512))\n",
    "\n",
    "# iterate over files and extract feature representations\n",
    "for n in range(n_files):\n",
    "    if n % 10 == 0:\n",
    "        print(f\"{n+1}/{n_files}\")\n",
    "    fn_wav = os.path.join(dir_dataset, df[\"fn_wav\"][n])\n",
    "    mfcc[n, :] = compute_mfcc(fn_wav)\n",
    "    emb[n, :] = compute_openl3(fn_wav)\n",
    "\n",
    "print(\"Feature extraction finished\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f497366",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "We'll create a simple Multilayer Perceptron (MLP) for genre classification and compare both feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_in, n_classes=10):\n",
    "    \"\"\" Simple MLP model \"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(n_in,))\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(inp)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    out = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd655455",
   "metadata": {},
   "source": [
    "## Music Genre Classification\n",
    "\n",
    "We want to evaluate our two audio feature representations (MFCC, OpenL3) for the **music genre classification** task.\n",
    "\n",
    "**Normaization**: We standardize both the training and test set using the **StandardScaler()** class from scikit-learn.\n",
    "\n",
    "**Train-Test-Split**: We split our dataset by using the first 7 files per genre as **training set** and the remaining 3 files as **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b027436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature standardization\n",
    "mfcc = StandardScaler().fit_transform(mfcc)\n",
    "emb = StandardScaler().fit_transform(emb)\n",
    "\n",
    "# file indices within each genre\n",
    "file_num = np.concatenate([np.arange(10) for _ in range(10)])\n",
    "\n",
    "# training & test set indices\n",
    "is_train = file_num < 7\n",
    "is_test = file_num >= 7\n",
    "\n",
    "mfcc_train = mfcc[is_train, :]\n",
    "mfcc_test = mfcc[is_test, :]\n",
    "emb_train = emb[is_train, :]\n",
    "emb_test = emb[is_test, :]\n",
    "\n",
    "# one-hot-encoded targets\n",
    "class_ids_train = class_ids[is_train]\n",
    "class_ids_test = class_ids[is_test]\n",
    "\n",
    "target_train = to_categorical(class_ids_train, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate MFCC\")\n",
    "model = create_model(n_in = mfcc_train.shape[1])\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "hist_mfcc = model.fit(mfcc_train, target_train, batch_size=4, epochs=300, verbose=0)\n",
    "target_pred_mfcc = model.predict(mfcc_test)\n",
    "class_id_pred_mfcc = np.argmax(target_pred_mfcc, axis=1)\n",
    "acc_mfcc = accuracy_score(class_ids_test, class_id_pred_mfcc)\n",
    "cm_mfcc = confusion_matrix(class_ids_test, class_id_pred_mfcc)\n",
    "\n",
    "print(\"Evaluate OpenL3\")\n",
    "model = create_model(n_in = emb_train.shape[1])\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "hist_emb = model.fit(emb_train, target_train, batch_size=4, epochs=300, verbose=0)\n",
    "target_pred_emb = model.predict(emb_test)\n",
    "class_id_pred_emb = np.argmax(target_pred_emb, axis=1)\n",
    "acc_emb = accuracy_score(class_ids_test, class_id_pred_emb)\n",
    "cm_emb = confusion_matrix(class_ids_test, class_id_pred_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b147cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training curves\n",
    "pl.figure(figsize=(5,3))\n",
    "pl.plot(hist_mfcc.history['accuracy'], label='MFCC')\n",
    "pl.plot(hist_emb.history['accuracy'], label='OpenL3')\n",
    "pl.legend()\n",
    "pl.xlabel('Epoch')\n",
    "pl.ylabel('Accuracy')\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_labels):\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # normalize\n",
    "    \n",
    "    # Plotting\n",
    "    sns.heatmap(cm_normalized, annot=True, ax=pl.gca(), cmap='Blues', cbar=False)\n",
    "\n",
    "    # labels, title and ticks\n",
    "    pl.gca().set_xlabel('Predicted')\n",
    "    pl.gca().set_ylabel('True')\n",
    "    pl.gca().xaxis.set_ticklabels(class_labels)\n",
    "    pl.gca().yaxis.set_ticklabels(class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(12,8))\n",
    "pl.subplot(1,2,1)\n",
    "plot_confusion_matrix(cm_mfcc, unique_genres)\n",
    "pl.title(f'MFCC (Accuracy = {acc_mfcc})')\n",
    "pl.subplot(1,2,2)\n",
    "plot_confusion_matrix(cm_emb, unique_genres)\n",
    "pl.title(f'OpenL3 (Accuracy = {acc_emb})')\n",
    "pl.tight_layout()\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9ffe1",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "The OpenL3 embeddings outperform the MFCC-based model, which is trained **from scratch**. The OpenL3 model was pre-trained on a larger dataset before and can use this knowledge..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed314e4",
   "metadata": {},
   "source": [
    "# Music Similarity\n",
    "\n",
    "Let's try both feature representations to implement a **music recommendation** algorithm. \n",
    "\n",
    "The idea is, given a **query song** (randomly chosen clip from our dataset), we compute the **distance** between the query song and all other files in the dataset by computing the **Euclidean distance** in the feature space (either MFCC or OpenL3).\n",
    "Then, we look for the *N* **closest songs in the feature space** (which will become the **recommended songs** most similar to the **query songs**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query song\n",
    "random_id = 66\n",
    "fn_wav = os.path.join(dir_dataset, df[\"fn_wav\"][random_id])\n",
    "genre = df[\"genre\"][random_id]\n",
    "print(f\"File: {fn_wav} - Music Genre: {genre}\")\n",
    "x, fs = librosa.load(fn_wav, sr=44100)\n",
    "ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d016e2",
   "metadata": {},
   "source": [
    "Let's compute the **Euclidean distance** between all songs and the query song..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mfcc = np.sqrt(np.sum((emb - emb[random_id])**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1966b8",
   "metadata": {},
   "source": [
    "... and sort the songs by distance (closer songs are presumably more similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca57d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(dist_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbed17",
   "metadata": {},
   "source": [
    "## Music recommendation scenario\n",
    "\n",
    "Let's take an arbitrary song and show the seven most closest songs in our dataset (this would be our recommendation result).\n",
    "\n",
    "You can **evaluate the results** with **two strategies**:\n",
    "1. Listen and see if you think they are similar to the query (**subjective evaluation**)\n",
    "2. Observe whether they come from the same music genre as the query (somewhat more **objective evaluation**), assuming that songs from the same genre are more similar to each other than songs from different genres..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = 66\n",
    "fn_wav = os.path.join(dir_dataset, df[\"fn_wav\"][query_id])\n",
    "genre = df[\"genre\"][random_id]\n",
    "print(f\"QUERY File: {fn_wav} - Music Genre: {genre}\")\n",
    "\n",
    "x, fs = librosa.load(fn_wav, sr=44100)\n",
    "ipd.display(ipd.Audio(data=x, rate=fs))\n",
    "\n",
    "for i in range(7):\n",
    "    \n",
    "    curr_idx = idx[i+1] \n",
    "    fn_wav = os.path.join(dir_dataset, df[\"fn_wav\"][curr_idx])\n",
    "    genre = df[\"genre\"][curr_idx]\n",
    "    print(f\"File: {fn_wav} - Music Genre: {genre} - Distance {dist_mfcc[idx[i+1]]}\")\n",
    "    x, fs = librosa.load(fn_wav, sr=44100)\n",
    "    ipd.display(ipd.Audio(data=x, rate=fs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d614e",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "- most of the retrieved songs also come from the Metal genre and show a lot of similarities in terms of instrumentation and rhythm with the query file :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58769e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
