{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "# Computational Analysis of Sound and Music\n",
    "\n",
    "# D3 - Deep Learning - Network Architectures\n",
    "\n",
    "Dr.-Ing. Jakob AbeÃŸer, jakob.abesser@idmt.fraunhofer.de\n",
    "\n",
    "**Last update:** 28.03.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690b683",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "In this notebook, you will learn how to use a convolutional neural network to solve the animal classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import IPython.display as ipd\n",
    "import wget\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa517f8",
   "metadata": {},
   "source": [
    "Let's make sure we have the dataset we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('animal_sounds.zip'):\n",
    "    print('Please wait a couple of seconds ...')\n",
    "    wget.download('https://github.com/machinelistening/machinelistening.github.io/blob/master/animal_sounds.zip?raw=true', \n",
    "                      out='animal_sounds.zip', bar=None)\n",
    "    print('animal_sounds.zip downloaded successfully ...')\n",
    "else:\n",
    "    print('Files already exist!')\n",
    "    \n",
    "if not os.path.isdir('animal_sounds'):\n",
    "    print(\"Let's unzip the file ... \")\n",
    "    assert os.path.isfile('animal_sounds.zip')\n",
    "    with zipfile.ZipFile('animal_sounds.zip', 'r') as f:\n",
    "        # Entpacke alle Inhalte in das angegebene Verzeichnis\n",
    "        f.extractall('.')\n",
    "    assert os.path.isdir('animal_sounds')\n",
    "    print(\"All done :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b858785",
   "metadata": {},
   "source": [
    "## Classifying animal sounds with a fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71160b",
   "metadata": {},
   "source": [
    "We'll use the animal classification dataset from the lecture on **Machine Learning**\n",
    "\n",
    "**Reminder**: The dataset used here is a manual selection of 5 examples for 5 animal classes from the https://github.com/karolpiczak/ESC-50 dataset.\n",
    "\n",
    "Let's use the code from before to load the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36751d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "dir_dataset = 'animal_sounds'\n",
    "sub_directories = glob.glob(os.path.join(dir_dataset, '*'))\n",
    "\n",
    "n_sub = len(sub_directories)\n",
    "# let's collect the files in each subdirectory\n",
    "# the folder name is the class name\n",
    "fn_wav_list = []\n",
    "class_label = []\n",
    "file_num_in_class = []\n",
    "\n",
    "for i in range(n_sub):\n",
    "    current_class_label = os.path.basename(sub_directories[i])\n",
    "    current_fn_wav_list = sorted(glob.glob(os.path.join(sub_directories[i], '*.wav')))\n",
    "    for k, fn_wav in enumerate(current_fn_wav_list):\n",
    "        fn_wav_list.append(fn_wav)\n",
    "        class_label.append(current_class_label)\n",
    "        file_num_in_class.append(k)\n",
    "\n",
    "n_files = len(class_label)\n",
    "   \n",
    "# this vector includes a \"counter\" for each file within its class, we use it later ...\n",
    "file_num_in_class = np.array(file_num_in_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d86391",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = sorted(list(set(class_label)))\n",
    "print(\"All unique class labels (sorted alphabetically)\", unique_classes)\n",
    "class_id = np.array([unique_classes.index(_) for _ in class_label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e076a",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Again, we'll use Mel spectrograms as feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea378c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mel_spec_for_audio_file(fn_wav,\n",
    "                                    n_fft=1024,  \n",
    "                                    hop_length=441,\n",
    "                                    fs=22050.,\n",
    "                                    n_mels=64):\n",
    "    \"\"\" Compute mel spectrogram\n",
    "    Args:\n",
    "        fn_wav (str): Audio file name\n",
    "        n_fft (int): FFT size\n",
    "        hop_length (int): Hop size in samples\n",
    "        fs (float): Sample rate in Hz\n",
    "        n_mels (int): Number of mel bands\n",
    "    \"\"\"\n",
    "    # load audio samples\n",
    "    x, fs = librosa.load(fn_wav, sr=fs, mono=True)\n",
    "\n",
    "    # normalize to the audio file to an maximum absolute value of 1\n",
    "    if np.max(np.abs(x)) > 0:\n",
    "        x = x / np.max(np.abs(x))\n",
    "\n",
    "    # mel-spectrogram\n",
    "    X = librosa.feature.melspectrogram(y=x,\n",
    "                                       sr=fs,\n",
    "                                       n_fft=n_fft,\n",
    "                                       hop_length=hop_length,\n",
    "                                       n_mels=n_mels,\n",
    "                                       fmin=0.0,\n",
    "                                       fmax=fs / 2,\n",
    "                                       power=1.0,\n",
    "                                       htk=True,\n",
    "                                       norm=None)\n",
    "\n",
    "    # apply dB normalization\n",
    "    X = librosa.amplitude_to_db(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0494ee8",
   "metadata": {},
   "source": [
    "Let's compute all mel spectrograms (*this takes a couple of seconds to compute*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mel_specs = []\n",
    "for i, fn_wav in enumerate(fn_wav_list):\n",
    "    all_mel_specs.append(compute_mel_spec_for_audio_file(fn_wav_list[i]))\n",
    "    \n",
    "print(\"We have {} spectrograms of shape {}\".format(len(all_mel_specs), all_mel_specs[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ab72b",
   "metadata": {},
   "source": [
    "The final shape of each spectrogram is ```(64, 251)``` and covers 64 mel frequency bands and 251 time frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1006e5f7",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "In contrast to the previous notebook, where we used a DNN for the animal classification task, we now want to set up a convolutional neural network (CNN) and use the Mel spectrograms as \"image-like\" input data. The intuition is that the CNN should be able to learn to recognize specific temporal-spectral patterns of each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mel_specs = np.array(all_mel_specs)\n",
    "print(f\"Shape of our data tensor : {all_mel_specs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62e267",
   "metadata": {},
   "source": [
    "The tensor dimensions are:\n",
    "- batch dimension (25) - we have 25 spectrogram examples\n",
    "- frequency dimension (64) - our Mel spectrogram has 64 Mel bands\n",
    "- time dimension (251) - the spectrograms have 251 time frames\n",
    "\n",
    "**Next Step**: For training the CNN, we would like to have **more examples**. We will randomly cut out 10 segments from each spectrogram (we assume that the typical animal sounds are audible not just once but rather throughout the recording) -> every segment will \"inherit\" the class label from its original spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_list = []\n",
    "segment_file_id = []\n",
    "segment_class_id = []\n",
    "segment_spec_id = []\n",
    "\n",
    "n_spectrograms = all_mel_specs.shape[0]\n",
    "\n",
    "n_segments_per_spectrogram = 10\n",
    "segment_length_frames = 100\n",
    "\n",
    "spec_length_frames = all_mel_specs.shape[2]\n",
    "max_segment_start_offset = spec_length_frames - segment_length_frames\n",
    "\n",
    "# iterate over all spectrograms\n",
    "for i in range(n_spectrograms):\n",
    "    \n",
    "    # create [n_segments_per_spectrogram] segments\n",
    "    for s in range(n_segments_per_spectrogram):\n",
    "        \n",
    "        # random segment start frame\n",
    "        segment_start_frames = int(np.random.rand(1)*max_segment_start_offset)\n",
    "    \n",
    "        segment_list.append(all_mel_specs[i, :, segment_start_frames:segment_start_frames+segment_length_frames])\n",
    "        \n",
    "        segment_file_id.append(i)\n",
    "        segment_class_id.append(class_id[i])\n",
    "        segment_spec_id.append(s)\n",
    "        \n",
    "# finally, let's convert our list of spectrogram segments again into a 3D tensor\n",
    "segment_list = np.array(segment_list)\n",
    "\n",
    "segment_file_id = np.array(segment_file_id)\n",
    "segment_file_mod_id = np.mod(segment_file_id, 5)\n",
    "\n",
    "segment_class_id = np.array(segment_class_id)\n",
    "segment_spec_id = np.array(segment_spec_id)\n",
    "\n",
    "\n",
    "print(f\"New data tensor shape {segment_list.shape}\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(12, 4))\n",
    "pl.plot(segment_file_id, 'b-', label='segment file ID')\n",
    "pl.plot(segment_file_mod_id, 'b--', label='segment file ID (per spectrogram)')\n",
    "pl.plot(segment_class_id, label='segment class ID')\n",
    "pl.plot(segment_spec_id, label='segment ID')\n",
    "pl.legend()\n",
    "pl.xlabel('Segment')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1ca67",
   "metadata": {},
   "source": [
    "**Obersevation**\n",
    "- The **segment_file_id** tells us, which of the 25 original spectrograms a segment comes from.\n",
    "- The **segment_file_mod_id** tells us, which of the 5 example spectrograms (from each animal class), the segment comes from.\n",
    "- The **class_id** tells us, which of the 5 animal classes a segment relates to.\n",
    "- The **segment_id** is an index from 0 to 9 (we have 10 segments per spectrogram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbf7f0",
   "metadata": {},
   "source": [
    "Let's have a closer look into our first dataset file (top) and the segments (below) that we extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(2.5, 2))\n",
    "pl.imshow(all_mel_specs[0, :, :], origin=\"lower\", aspect=\"auto\", interpolation=\"None\")\n",
    "pl.xticks([], [])\n",
    "pl.yticks([], [])\n",
    "pl.title('Original spectrogram')\n",
    "pl.tight_layout()\n",
    "pl.show()\n",
    "\n",
    "pl.figure(figsize=(15,5))\n",
    "ny = 2\n",
    "nx = int(n_segments_per_spectrogram // ny)\n",
    "for s in range(n_segments_per_spectrogram):\n",
    "    pl.subplot(ny, nx, s+1)\n",
    "    pl.imshow(segment_list[s, :, :], origin=\"lower\", aspect=\"auto\", interpolation=\"None\")\n",
    "    if s == 0:\n",
    "        pl.title('Extracted segments')\n",
    "    pl.xticks([], [])\n",
    "    pl.yticks([], [])\n",
    "pl.tight_layout()\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35c27c",
   "metadata": {},
   "source": [
    "**Obersevation**: Our code works. We extracted 10 segments at random time positions. All of these segments show characteristic properties:\n",
    "  - fundamental & harmonic frequencies\n",
    "  - time-varying frequency contours\n",
    "  - background noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ba4cd",
   "metadata": {},
   "source": [
    "### Dataset split into training and test set\n",
    "\n",
    "We'll start with the code from the previous notebook. Again, we take the first three files per class as training data and the final two files as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa349a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = np.where(segment_file_mod_id <= 2)[0]\n",
    "is_test = np.where(segment_file_mod_id >= 3)[0]\n",
    "\n",
    "print(\"Our feature matrix is split into {} training examples and {} test examples\".format(len(is_train), len(is_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3c49b",
   "metadata": {},
   "source": [
    "Now that we have splitted our dataset, we can generate the feature matrix and target vectors for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = segment_list[is_train, :, :]\n",
    "y_train = segment_class_id[is_train]\n",
    "X_test = segment_list[is_test, :, :]\n",
    "y_test = segment_class_id[is_test]\n",
    "\n",
    "print(\"Let's look at the dimensions\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3361b62",
   "metadata": {},
   "source": [
    "Let's normalize our feature matrix first, this makes sure that all feature dimensions (columns in the feature matrix) have a mean of 0 and a standard deviation of 1. This makes it easier for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d70154",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = np.zeros_like(X_train)\n",
    "X_test_norm = np.zeros_like(X_test)\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_norm[i, :, :] = StandardScaler().fit_transform(X_train[i, :, :])\n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_norm[i, :, :] = StandardScaler().fit_transform(X_test[i, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43e24",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Similar to the previous notebook, we have to one-hot-encode our class labels for all segments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_transformed = OneHotEncoder(categories='auto', sparse=False).fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_transformed = OneHotEncoder(categories='auto', sparse=False).fit_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8997e95",
   "metadata": {},
   "source": [
    "### Channel dimension\n",
    "\n",
    "As a final step, we'll add a fourth (\"singleton\") dimension to our training and test feature tensors, which mean that we only use one channel.\n",
    "\n",
    "**Remember**: For image classification tasks, we would use three channels to separately encode the red, green, and blue color channels (RGB channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5544b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_train_norm.shape) == 3:\n",
    "    X_train_norm = np.expand_dims(X_train_norm, -1)\n",
    "    X_test_norm = np.expand_dims(X_test_norm, -1)\n",
    "\n",
    "else:\n",
    "    print(\"We already have four dimensions\")\n",
    "    \n",
    "print(f\"Let's check if we have four dimensions. New shapes: {X_train_norm.shape} & {X_test_norm.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d287b8",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "#### Create Model\n",
    "\n",
    "Let's build our neural network. We will continue to use the [Sequential API](https://keras.io/guides/sequential_model/) for doing this easily. Later, when we use more complex models, we use the [Functional API](https://keras.io/guides/functional_api/) to do this more flexibly.\n",
    "\n",
    "We will need the following ingredients:\n",
    "  - the ```Input``` layer defines the input shape that the CNN can expect. This is the shape of the segments in our feature matrix\n",
    "  - the convolutional layers (```Conv2D```) are the core part of the CNN. They learn specific filters to recognitize the animals based on characteristic patterns in the spectrogram\n",
    "  - each convolutional layer will be followed by a pooling layer (```MaxPooling2D```) to reduce the spatial dimensions of the output. \n",
    "  - finally, we will add a fully connected layer (```Dense```) for the classification output.\n",
    "\n",
    "We furthermore use the ```Sequential``` model, which allows to easily add more layers to a network.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# The input shape is the \"time-frequency shape\" of our segments + the number of channels\n",
    "# Make sure to NOT include the first (batch) dimension!\n",
    "input_shape = X_train_norm.shape[1:]\n",
    "\n",
    "# Get the number of classes:\n",
    "n_classes = y_train_transformed.shape[1]\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "# Flatten the output to feed into the Dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Output layers for classification\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=n_classes, activation='softmax')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab9667",
   "metadata": {},
   "source": [
    "Let's have a look into the architecture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452b916",
   "metadata": {},
   "source": [
    "Finally, let's compile the model. Then we're ready for model training :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ec14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40f71e",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "\n",
    "Now we train the model over $50$ iterations using our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_norm, y_train_transformed, epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8a6f0",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "- We see that the **loss** value is decreasing and at the same time the **accuracy** is increasing, which show us that the model learns to classify the **training data** better and better over the first 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8, 4))\n",
    "pl.subplot(2,1,1)\n",
    "pl.plot(history.history['loss'])\n",
    "pl.ylabel('Loss')\n",
    "pl.subplot(2,1,2)\n",
    "pl.plot(history.history['accuracy'])\n",
    "pl.ylabel('Accuracy (Training Set)')\n",
    "pl.xlabel('Epoch')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05332658",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "\n",
    "Now we evaluate the model using the **test data** and compute the **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_norm)\n",
    "\n",
    "print(\"Shape of the predictions: {}\".format(y_test_pred.shape))\n",
    "\n",
    "# The model outputs in each row 5 probability values (they always add to 1!) for each class. \n",
    "# We want take the class with the highest probability as prediction!\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "print(\"Shape of the predictions now: {}\".format(y_test_pred.shape))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy score = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9384b",
   "metadata": {},
   "source": [
    "For the **evaluation**, we will compute the **accuracy** and the **confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_test, y_test_pred).astype(np.float32)\n",
    "# normalize to probabilities\n",
    "for i in range(cm.shape[0]):\n",
    "    if np.sum(cm[i, :]) > 0:\n",
    "        cm[i, :] /= np.sum(cm[i, :])  # by dividing through the sum, we convert counts to probabilities\n",
    "pl.imshow(cm)\n",
    "ticks = np.arange(5)\n",
    "pl.xticks(ticks, unique_classes)\n",
    "pl.yticks(ticks, unique_classes)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496533f6",
   "metadata": {},
   "source": [
    "**How to further improve this algorithm?**\n",
    "  - for now, we computed the predictions **per segment**, we can easily aggregate them **to file-level classifications** by **averaging over the frame-level probabilies**\n",
    "  - change the number and kernel size for the Conv2D layers\n",
    "  - try to integrate Dropout after the MaxPooling layers as well\n",
    "  - try to train the model for a longer time (more epochs)\n",
    "  - try another optimizer (see https://analyticsindiamag.com/guide-to-tensorflow-keras-optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e28d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
