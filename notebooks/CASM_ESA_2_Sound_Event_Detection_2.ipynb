{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "## Computational Analysis of Sound and Music\n",
    "\n",
    "# ESA 2 - Sound Event Detection - 2\n",
    "\n",
    "Dr.-Ing. Jakob Abe√üer, jakob.abesser@idmt.fraunhofer.de\n",
    "\n",
    "**Last update:** 25.05.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded3d3a",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "In this notebook, we revise the M1 notebook and use a small dataset of **animal sounds** extracted from the **ESC50 dataset**.\n",
    "We will study how to \n",
    "- apply the data augmentation **before** the training to enhance our training set\n",
    "- apply the data augmentation **during** the training in a custom **generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1612f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import os\n",
    "import matplotlib\n",
    "import librosa\n",
    "import matplotlib.pyplot as pl\n",
    "import platform\n",
    "import IPython.display as ipd\n",
    "import wget\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b858785",
   "metadata": {},
   "source": [
    "## Dataset download & pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('animal_sounds.zip'):\n",
    "    print('Please wait a couple of seconds ...')\n",
    "    wget.download('https://github.com/machinelistening/machinelistening.github.io/blob/master/animal_sounds.zip?raw=true', \n",
    "                      out='animal_sounds.zip', bar=None)\n",
    "    print('animal_sounds.zip downloaded successfully ...')\n",
    "else:\n",
    "    print('Files already exist!')\n",
    "    \n",
    "if not os.path.isdir('animal_sounds'):\n",
    "    print(\"Let's unzip the file ... \")\n",
    "    assert os.path.isfile('animal_sounds.zip')\n",
    "    with zipfile.ZipFile('animal_sounds.zip', 'r') as f:\n",
    "        # Entpacke alle Inhalte in das angegebene Verzeichnis\n",
    "        f.extractall('.')\n",
    "    assert os.path.isdir('animal_sounds')\n",
    "    print(\"All done :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample rate\n",
    "fs = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36751d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the subdirectories (which provide us the animal classes)\n",
    "dir_dataset = 'animal_sounds'\n",
    "sub_directories = glob.glob(os.path.join(dir_dataset, '*'))\n",
    "\n",
    "n_sub = len(sub_directories)\n",
    "# let's collect the files in each subdirectory\n",
    "# the folder name is the class name\n",
    "fn_wav_list = []\n",
    "class_label = []\n",
    "file_num_in_class = []\n",
    "\n",
    "for i in range(n_sub):\n",
    "    current_class_label = os.path.basename(sub_directories[i])\n",
    "    current_fn_wav_list = sorted(glob.glob(os.path.join(sub_directories[i], '*.wav')))\n",
    "    for k, fn_wav in enumerate(current_fn_wav_list):\n",
    "        fn_wav_list.append(fn_wav)\n",
    "        class_label.append(current_class_label)\n",
    "        file_num_in_class.append(k)\n",
    "\n",
    "n_files = len(class_label)\n",
    "    \n",
    "# this vector includes a \"counter\" for each file within its class, we use it later ...\n",
    "file_num_in_class = np.array(file_num_in_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d86391",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = sorted(list(set(class_label)))\n",
    "class_id = np.array([unique_classes.index(_) for _ in class_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534a702",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3535637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melspec(fn_wav, n_bins=128):\n",
    "    \"\"\" Compute Mel spectrogram with logarithmic magnitude scaling \n",
    "    Args:\n",
    "        fn_wav (str): WAV file name\n",
    "        n_bins (int): Number of Mel frequency bins\n",
    "    Returns:\n",
    "        mel_spec (2d np.ndarray): Mel spectrogram (n_bins x n_frames)\n",
    "    \"\"\"\n",
    "    x, fs = librosa.load(fn_wav, mono=True, sr=44100)\n",
    "    S = librosa.feature.melspectrogram(y=x, sr=fs, n_mels=n_bins, fmax=fs/2)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096caaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = []\n",
    "for fn_wav in fn_wav_list:\n",
    "    feat.append(compute_melspec(fn_wav))\n",
    "feat = np.array(feat)\n",
    "feat = np.expand_dims(feat, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e60440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature matrix shape: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7131ab",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa349a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = np.where(file_num_in_class <= 2)[0]\n",
    "is_test = np.where(file_num_in_class >= 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feat[is_train, :, :, :]\n",
    "X_test = feat[is_test, :, :, :]\n",
    "\n",
    "y_train = class_id[is_train]\n",
    "y_test = class_id[is_test]\n",
    "\n",
    "# one-hot-encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=5)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=5)\n",
    "\n",
    "# Data standardization\n",
    "X_train -= np.mean(X_train)\n",
    "X_train /= np.std(X_train)\n",
    "\n",
    "X_test -= np.mean(X_test)\n",
    "X_test /= np.std(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3361b62",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "We use the same CNN model as in the previous seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d16f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creage_vgg_like_model(input_shape, num_output_dim):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    x = None\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            x = inp\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([tf.keras.layers.GlobalAveragePooling2D()(x),\n",
    "                                     tf.keras.layers.GlobalMaxPooling2D()(x)])\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    out = tf.keras.layers.Dense(num_output_dim, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    " \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19265d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all training set files and save audio samples in a 2D array\n",
    "all_samples = []\n",
    "for fn_wav in fn_wav_list:\n",
    "    x, fs = librosa.load(fn_wav, mono=True, sr=44100)\n",
    "    all_samples.append(x)\n",
    "all_samples = np.vstack(all_samples)\n",
    "all_samples_train = all_samples[is_train, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbbff6",
   "metadata": {},
   "source": [
    "## Data Augmentation Strategy 1: Enhance training dataset (before the training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b7722",
   "metadata": {},
   "source": [
    "### Data Augmentation of Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.03, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d586d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_augmenations_per_file = 10\n",
    "n_files = all_samples_train.shape[0]\n",
    "\n",
    "samples_train_aug = []\n",
    "y_train_s1 = []\n",
    "for i in range(n_files):\n",
    "    for k in range(n_augmenations_per_file):\n",
    "        # create augmented version\n",
    "        samples_train_aug.append(augment(all_samples_train[i], fs))\n",
    "        # clone target\n",
    "        y_train_s1.append(y_train[i, :])\n",
    "\n",
    "samples_train_aug = np.vstack(samples_train_aug)\n",
    "y_train_s1 = np.vstack(y_train_s1)\n",
    "\n",
    "print(samples_train_aug.shape)\n",
    "print(y_train_s1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20113950",
   "metadata": {},
   "source": [
    "**Observation**: Since we have generated 10 augmented versions of our initial files, we now have a total of **150 audio clips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee283c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Mel spectrograms for 150 files\n",
    "n_clips = samples_train_aug.shape[0]\n",
    "X_train_s1 = []\n",
    "for i in range(n_clips):\n",
    "    spec = librosa.feature.melspectrogram(y=samples_train_aug[i, :], sr=fs, n_mels=128, fmax=fs/2)\n",
    "    spec = librosa.power_to_db(spec, ref=np.max)\n",
    "    X_train_s1.append(spec)\n",
    "X_train_s1 = np.array(X_train_s1)\n",
    "X_train_s1 = np.expand_dims(X_train_s1, axis=-1)\n",
    "print(X_train_s1.shape)\n",
    "\n",
    "# Data standardization\n",
    "X_train_s1 -= np.mean(X_train_s1)\n",
    "X_train_s1 /= np.std(X_train_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de42a0e",
   "metadata": {},
   "source": [
    "### Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ad572",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_s1.shape[1:] \n",
    "model_s1 = creage_vgg_like_model(input_shape, 5)\n",
    "model_s1.fit(X_train_s1, y_train_s1, batch_size=2, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a691a",
   "metadata": {},
   "source": [
    "## Data Augmentation Strategy 2: Implement custom generator (data augmentation during the training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313c860",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "\n",
    "Instead of just calling the ```model.fit()``` function as before, we implement a **data generator**, which allows us to modify the training data **in each epoch**.\n",
    "\n",
    "The **generator class** includes the following **methods**:\n",
    "- ```__init__()```: Class constructor, we provide the training data and training targets as arguments, which will be stored in class member variables to be accessible from all functions\n",
    "- ```__len__()```: Method allows Keras to \"ask\" the generator, how many training data instances exist. This is important so Keras knows how many steps per training epoch it needs to run\n",
    "- ```__getitem__(k)```: Method called by Keras to get the data and target tuple used in the k-th step of the current epoch. This is the main method we need to implement where the original training data can be modified (data augmentation)\n",
    "- ```on_epoch_end()```: This method is always called after all steps of one epoch were executed. We'll use it to randomize the order of all data instances in every epoch.\n",
    "\n",
    "**Note**: For simplicity, we implement here a **batchsize = 1** (so every step uses one spectrogram as features, not multiple ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1883a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, audio_samples, targets):\n",
    "        # store audio_samples and target as member variables\n",
    "        self.audio_samples = audio_samples\n",
    "        self.targets = targets\n",
    "\n",
    "        # derive the number of files \n",
    "        self.n_files = self.audio_samples.shape[0]\n",
    "        self.n_samples = self.audio_samples.shape[1]\n",
    "        \n",
    "        # array of file indexes that we can shuffle after each training epoch to use files in random order\n",
    "        self.indexes = np.arange(self.n_files)\n",
    "        \n",
    "        # sample rate\n",
    "        self.fs = 44100\n",
    "        \n",
    "        # prepare data augmentation using audiomentations\n",
    "        self.augment = Compose([AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.005, p=0.5),\n",
    "                                TimeStretch(min_rate=0.95, max_rate=1.05, p=0.5)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the number of training examples \"\"\"\n",
    "        return self.n_files\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # get current audio samples\n",
    "        curr_samples = self.audio_samples[self.indexes[index], :]\n",
    "        \n",
    "        # apply data augmentation\n",
    "        curr_samples_aug = self.augment(samples=curr_samples, sample_rate=44100)\n",
    "        \n",
    "        # compute Mel spectrogram\n",
    "        spec = librosa.feature.melspectrogram(y=curr_samples, sr=self.fs, n_mels=128, fmax=self.fs/2)\n",
    "        spec = librosa.power_to_db(spec, ref=np.max)\n",
    "        \n",
    "        # standardize\n",
    "        spec -= np.mean(spec)\n",
    "        spec /= np.std(spec)\n",
    "\n",
    "        # convert 2D spectrogram (frequency x time) into \n",
    "        # 4D data tensor (batch x frequency x time x channels)\n",
    "        # remember: batchsize = 1 and number of channels = 1\n",
    "        feat = np.zeros((1, spec.shape[0], spec.shape[1], 1))\n",
    "        feat[0, :, :, 0] = spec\n",
    "        \n",
    "        # 2D target tensor (batch x number of classes), here: (1 x number of classes)\n",
    "        target = self.targets[self.indexes[index], :]\n",
    "        target = np.expand_dims(target, axis=0)\n",
    "        \n",
    "        return feat, target\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # shuffle training file indeces\n",
    "        np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff21d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "generator = DataGenerator(all_samples_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8b7e7",
   "metadata": {},
   "source": [
    "### Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c9701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_s2 = creage_vgg_like_model(input_shape, 5)\n",
    "\n",
    "# train model using generator\n",
    "model_s2.fit(generator, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3b47e",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate both models on the test set\n",
    "acc = np.zeros(2)\n",
    "for i, model in enumerate((model_s1, model_s2)):\n",
    "\n",
    "    # evaluate model on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    class_id_test = np.argmax(y_test, axis=1)\n",
    "    class_id_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "    acc[i] = accuracy_score(class_id_test, class_id_test_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure()\n",
    "pl.plot(acc, 'o-')\n",
    "pl.xticks((0, 1), ('S1', 'S2'))\n",
    "pl.ylabel('Accuracy')\n",
    "pl.xlabel('Strategy')\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
