{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "# Computational Analysis of Sound and Music\n",
    "\n",
    "# MIR 3 - Music Transcription - 1\n",
    "\n",
    "Dr.-Ing. Jakob AbeÃŸer, jakob.abesser@idmt.fraunhofer.de\n",
    "\n",
    "**Last update:** 12.05.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690b683",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "In this notebook, you will learn how to implement a simple **multipitch estimation** algorithm using a **U-Net deep neural network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import wget\n",
    "import zipfile\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, concatenate, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eda528",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa517f8",
   "metadata": {},
   "source": [
    "We need a small dataset of piano recordings with corresponding multipitch annotations.\n",
    "\n",
    "We use 4 files of the **SMD Dataset** (https://www.audiolabs-erlangen.de/resources/MIR/SMD/midi):\n",
    "- Bach_BWV888-02_008_20110315-SMD\n",
    "- Beethoven_Op027No1-02_003_20090916-SMD\n",
    "- Chopin_Op028-01_003_20100611-SMD\n",
    "- Chopin_Op028-03_003_20100611-SMD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('SMD.zip'):\n",
    "    print('Please wait a couple of seconds ...')\n",
    "    wget.download('https://github.com/machinelistening/machinelistening.github.io/blob/master/SMD.zip?raw=true', \n",
    "                      out='SMD.zip', bar=None)\n",
    "    print('SMD downloaded successfully ...')\n",
    "else:\n",
    "    print('Files already exist!')\n",
    "    \n",
    "if not os.path.isdir('SMD.zip'):\n",
    "    print(\"Let's unzip the file ... \")\n",
    "    assert os.path.isfile('SMD.zip')\n",
    "    with zipfile.ZipFile('SMD.zip', 'r') as f:\n",
    "        # Entpacke alle Inhalte in das angegebene Verzeichnis\n",
    "        f.extractall('.')\n",
    "    assert os.path.isdir('SMD')\n",
    "    print(\"All done :)\")\n",
    "\n",
    "dir_dataset = 'SMD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796fda1",
   "metadata": {},
   "source": [
    "Let's check our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the directory\n",
    "files = os.listdir(dir_dataset)\n",
    "\n",
    "# Print the list of files\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2baa82",
   "metadata": {},
   "source": [
    "We have **4 audio files (WAV)** and **4 CSV files** with the corresponding pitch annotation.\n",
    "\n",
    "Let's just memorize the 4 file prefixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefixes = sorted(list(set([_[:-4] for _ in files])))\n",
    "print(file_prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7cab3",
   "metadata": {},
   "source": [
    "Let's listen to the first file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_wav = os.path.join(dir_dataset, file_prefixes[0] + '.wav')\n",
    "x, fs = librosa.load(fn_wav)\n",
    "ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c56c2",
   "metadata": {},
   "source": [
    "**Observation**: This a polyphonic piano recording sampled at 22.05kHz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92d6d5",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a44ec",
   "metadata": {},
   "source": [
    "We need a function to compute the **Constant-Q transform** from all these files. \n",
    "In contrast to the pitch tracking example from the last seminar, we only use one frequency bin per semitone, so 12 bins per octave.\n",
    "\n",
    "We want to cover the piano range between MIDI pitch 21 to 108 (https://newt.phys.unsw.edu.au/jw/notes.html), which covers the entire pitch range of a piano. This results in a minimum frequency of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min = 440 * 2**((21 - 69)/12)\n",
    "print(f\"f_min =  {f_min}Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0047c8a",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 108-21+1\n",
    "print(f\"{n_bins} frequency bins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1148a11",
   "metadata": {},
   "source": [
    "We furthermore need an array that contains the MIDI pitch values that correspond to each frequency bin in the CQT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b494e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqt_midi_pitch = np.arange(21, 109)\n",
    "print(cqt_midi_pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ed16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cqt(fn_wav, fs=44100, hop_length=256, bpo=12, fmin=31, nbins = 88):\n",
    "    \"\"\"\n",
    "    Compute the Constant-Q transform (CQT) with librosa\n",
    "    Args:\n",
    "        fn_wav (str): File path to the input audio file (in .wav format).\n",
    "        fs (int, optional): Sampling rate of the audio file (in Hz). Default is 22050 Hz.\n",
    "        hop_length (int, optional): Number of samples between successive CQT columns. Default is 256.\n",
    "        bpo (int, optional): Number of bins per octave for the CQT. Default is 12.\n",
    "        fmin (int, optional): Minimum frequency (in Hz) of the CQT. Default is 31 Hz.\n",
    "        nbins (int, optional): Number of bins in the CQT. Default is 320.\n",
    "\n",
    "    Returns:\n",
    "        cqt (np.ndarray): 2D array containing the magnitude CQT coefficients.\n",
    "        time_axis (np.ndarray): Numpy array with frame times in seconds.\n",
    "        freq_hz (np.ndarray): Frequency values in Hz of all frequency bands of the CQT.\n",
    "    \"\"\"\n",
    "    x, fs = librosa.load(fn_wav, sr=fs, mono=True)\n",
    "    cqt = np.abs(librosa.core.cqt(x, sr=fs, \n",
    "                                  hop_length=hop_length, \n",
    "                                  fmin=fmin,\n",
    "                                  bins_per_octave=bpo,\n",
    "                                  n_bins=nbins))\n",
    "    cqt = librosa.amplitude_to_db(cqt, ref=np.max)\n",
    "    cqt = cqt.astype(np.float16)\n",
    "    time_axis = np.arange(cqt.shape[1])*hop_length/fs\n",
    "    freq_hz = librosa.cqt_frequencies(n_bins=nbins, fmin=fmin, bins_per_octave=bpo)\n",
    "\n",
    "    return cqt, time_axis, freq_hz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d88316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try it:\n",
    "cqt, cqt_times_sec, cqt_freqs = compute_cqt(fn_wav)\n",
    "\n",
    "print(f\"Shape of CQT matrix {cqt.shape}\")\n",
    "\n",
    "# plot the first part of the CQT\n",
    "pl.figure(figsize=(10,3))\n",
    "librosa.display.specshow(cqt[:, :1000], sr=44100, x_axis='time', y_axis='cqt_note', ax=pl.gca())\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c36e8",
   "metadata": {},
   "source": [
    "**Observation**: We can see that the recording starts with a **monophonic** piano line until around 5s, then it becomes **polyphonic**, i.e., multiple notes are audible at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716ed72",
   "metadata": {},
   "source": [
    "## Import pitch annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the first CSV file\n",
    "fn_csv = os.path.join(dir_dataset, file_prefixes[0] + '.csv')\n",
    "data = np.loadtxt(fn_csv, delimiter=';', skiprows=1, usecols=(0, 1, 2))\n",
    "\n",
    "onset_sec = data[:, 0]\n",
    "duration_sec = data[:, 1]\n",
    "pitch = data[:, 2].astype(int)\n",
    "\n",
    "# let's check the first three notes\n",
    "print(onset_sec[:3])\n",
    "print(duration_sec[:3])\n",
    "print(pitch[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81e159",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "\n",
    "The first column includes **note onset times in seconds**, the second column **the note duration in seconds**, while the third column contains the **pitch encoded as MIDI pitch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ccacd",
   "metadata": {},
   "source": [
    "## Annotation Mapping\n",
    "\n",
    "**Problem**: \n",
    "\n",
    "We have a) our multipitch annotations and b) the time axis of our CQT, which are **not matching**.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "We want a binary matrix of the same shape as our **CQT** matrix, which has **ones** wherever a pitched frame is annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b913cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cqt_target(cqt_times_sec, cqt_midi_pitch, note_onset_sec, note_duration_sec, note_pitch):\n",
    "    \n",
    "    n_frames = len(cqt_times_sec)\n",
    "    n_bins = len(cqt_midi_pitch)\n",
    "    \n",
    "    cqt_target = np.zeros((n_bins, n_frames), dtype=bool)\n",
    "    \n",
    "    n_notes = len(note_pitch)\n",
    "    print(n_notes)\n",
    "    for i in range(n_notes):\n",
    "        \n",
    "        # find closest frequency bin\n",
    "        pitch = np.where(cqt_midi_pitch == note_pitch[i])[0]\n",
    "\n",
    "        start_frame = np.argmin(np.abs(cqt_times_sec-note_onset_sec[i]))\n",
    "        end_frame = np.argmin(np.abs(cqt_times_sec-(note_onset_sec[i]+note_duration_sec[i])))\n",
    "        \n",
    "        # \"mark\" it as \"True\"\n",
    "        cqt_target[pitch, start_frame:end_frame] = True\n",
    "            \n",
    "    return cqt_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a67afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try this again for the first song\n",
    "fn_wav = os.path.join(dir_dataset, file_prefixes[0] + '.wav')\n",
    "cqt, cqt_times_sec, cqt_freq_hz = compute_cqt(fn_wav)\n",
    "\n",
    "# load annotation file\n",
    "fn_csv = os.path.join(dir_dataset, file_prefixes[0] + '.csv')\n",
    "data = np.loadtxt(fn_csv, delimiter=';', skiprows=1, usecols=(0, 1, 2))\n",
    "onset_sec = data[:, 0]\n",
    "duration_sec = data[:, 1]\n",
    "pitch = data[:, 2].astype(int)\n",
    "\n",
    "# create targets for CQT matrix\n",
    "cqt_target = create_cqt_target(cqt_times_sec, cqt_midi_pitch, onset_sec, duration_sec, pitch)\n",
    "\n",
    "# finally, let's plot it \n",
    "pl.figure(figsize=(15,5))\n",
    "pl.subplot(1,2,1)\n",
    "librosa.display.specshow(cqt[:, :1000], sr=44100, x_axis='time', y_axis='cqt_note', ax=pl.gca())\n",
    "pl.title('CQT')\n",
    "pl.subplot(1,2,2)\n",
    "pl.imshow(cqt_target[:, :1000], aspect=\"auto\", origin=\"lower\", cmap=\"Greys\")\n",
    "pl.title('Target')\n",
    "pl.tight_layout()\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b267d",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The **piano-roll** target on the right the active pitches over time. This is what we want our model to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548d9d5",
   "metadata": {},
   "source": [
    "## Batch Feature Extraction\n",
    "\n",
    "Nice, so let's compute all 4 CQT spectrograms and all 4 target matrices: (**this takes some seconds ...**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ed06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cqts = []\n",
    "all_targets = []\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"Process file {i+1}/4\")\n",
    "    # (1) compute CQT\n",
    "    fn_wav = os.path.join(dir_dataset, file_prefixes[i] + '.wav')\n",
    "    cqt, cqt_times_sec, cqt_freq_hz = compute_cqt(fn_wav)\n",
    "    \n",
    "    # (2) compute target matrix\n",
    "    fn_csv = os.path.join(dir_dataset, file_prefixes[i] + '.csv')\n",
    "    data = np.loadtxt(fn_csv, delimiter=';', skiprows=1, usecols=(0, 1, 2))\n",
    "    onset_sec = data[:, 0]\n",
    "    duration_sec = data[:, 1]\n",
    "    pitch = data[:, 2].astype(int)\n",
    "    cqt_target = create_cqt_target(cqt_times_sec, cqt_midi_pitch, onset_sec, duration_sec, pitch)\n",
    "    \n",
    "    all_cqts.append(cqt)\n",
    "    all_targets.append(cqt_target)\n",
    "\n",
    "# we'll use the first three files as training and the final file as test set\n",
    "all_cqts_train = np.hstack(all_cqts[:3])\n",
    "all_targets_train = np.hstack(all_targets[:3])\n",
    "\n",
    "all_cqts_test = all_cqts[3]\n",
    "all_targets_test = all_targets[3]\n",
    "\n",
    "print(f\"Final shape (training) = {all_cqts_train.shape}\")\n",
    "print(f\"Final shape (test) = {all_cqts_test.shape}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d6c17",
   "metadata": {},
   "source": [
    "Let's **visualize** all CQTs and target matrices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8, 15))\n",
    "n = len(all_cqts)\n",
    "for i in range(n):\n",
    "    pl.subplot(2*n, 1, 2*i+1)\n",
    "    pl.imshow(all_cqts[i], aspect=\"auto\")\n",
    "    pl.axis(\"off\")    \n",
    "    pl.subplot(2*n, 1, 2*i+2)\n",
    "    pl.imshow(all_targets[i], aspect=\"auto\")\n",
    "    pl.axis(\"off\")\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2479c95",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4c393",
   "metadata": {},
   "source": [
    "### Patches\n",
    "\n",
    "For simplicity, well cut our matrices into patches of a fixed size (256 frames) with no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673cd66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train = []\n",
    "target_train = []\n",
    "n_frames = all_cqts_train.shape[1]\n",
    "offset = 0\n",
    "patch_len = 256\n",
    "patch_hop = 256\n",
    "\n",
    "while True:\n",
    "    if offset + patch_len >= n_frames:\n",
    "        break\n",
    "    feat_train.append(all_cqts_train[:, offset: offset + patch_len])\n",
    "    target_train.append(all_targets_train[:, offset: offset + patch_len])\n",
    "    offset += patch_hop\n",
    "feat_train = np.stack(feat_train)\n",
    "target_train = np.stack(target_train)\n",
    "\n",
    "# finally, we'll add a singleton dimension (one channel)\n",
    "feat_train = np.expand_dims(feat_train, -1)\n",
    "target_train = np.expand_dims(target_train, -1)\n",
    "\n",
    "print(f\"Feature & target array shape = {feat_train.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5fe33",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "We're using the **functional API** (https://keras.io/guides/functional_api/) to set up a **simple UNet** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73337fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size, padding=\"same\", act=\"relu\"):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, kernel_size, padding=padding, activation=act)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet(input_shape):\n",
    "    # Define input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    n_filters_encoder = [16, 32, 64]\n",
    "    n_filters_decoder = [64, 32, 1]\n",
    "    \n",
    "    # (1) Encoder\n",
    "    cb_enc_1 = conv_block(inputs, filters=n_filters_encoder[0], kernel_size=(3,3), padding=\"same\")\n",
    "    mp_enc_1 = MaxPool2D(pool_size=(2, 2))(cb_enc_1)\n",
    "    \n",
    "    cb_enc_2 = conv_block(mp_enc_1, filters=n_filters_encoder[0], kernel_size=(3,3), padding=\"same\")\n",
    "    mp_enc_2 = MaxPool2D(pool_size=(2, 2))(cb_enc_2)\n",
    "    \n",
    "    cb_enc_3 = conv_block(mp_enc_2, filters=n_filters_encoder[0], kernel_size=(3,3), padding=\"same\")\n",
    "    mp_enc_3 = MaxPool2D(pool_size=(2, 2))(cb_enc_3)\n",
    "    \n",
    "    # (2) Decoder\n",
    "    us_dec_1 = UpSampling2D(size=(2, 2))(mp_enc_3)\n",
    "    us_dec_1 = concatenate([us_dec_1, cb_enc_3])                      \n",
    "    cb_dec_1 = conv_block(us_dec_1, filters=n_filters_decoder[0], kernel_size=(3,3), padding=\"same\")\n",
    "\n",
    "    us_dec_2 = UpSampling2D(size=(2, 2))(cb_dec_1)\n",
    "    us_dec_2 = concatenate([us_dec_2, cb_enc_2])                      \n",
    "    cb_dec_2 = conv_block(us_dec_2, filters=n_filters_decoder[1], kernel_size=(3,3), padding=\"same\")\n",
    "\n",
    "    us_dec_3 = UpSampling2D(size=(2, 2))(cb_dec_2)\n",
    "    print(us_dec_3.shape, cb_enc_1.shape)\n",
    "    us_dec_3 = concatenate([us_dec_3, cb_enc_1])                      \n",
    "    cb_dec_3 = conv_block(us_dec_3, filters=n_filters_decoder[2], kernel_size=(3,3), padding=\"same\", act=\"sigmoid\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=cb_dec_3)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam')\n",
    " \n",
    "    return model\n",
    "    \n",
    "# Example usage\n",
    "input_shape = feat_train.shape[1:] \n",
    "model = create_unet(input_shape)\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5194653c",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(feat_train, target_train, epochs=30, batch_size=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(history.history['loss'])\n",
    "pl.ylabel('Loss')\n",
    "pl.xlabel('Epoch')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b21f18",
   "metadata": {},
   "source": [
    "## Model Test\n",
    "\n",
    "We're testing our model by making predictions on the fourth file.\n",
    "The approach is to \n",
    " - cut in into similarly sized patches \n",
    " - compute prediction for each patch\n",
    " - concatenate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_cqts_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84854e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = []\n",
    "offset = 0\n",
    "n_frames = all_cqts_test.shape[1]\n",
    "while True:\n",
    "    patch = all_cqts_test[:, offset:offset+patch_len]\n",
    "    patch = np.expand_dims(patch, 0)\n",
    "    patch = np.expand_dims(patch, -1)\n",
    "    \n",
    "    # compute model prediction for current \n",
    "    curr_pred = model.predict(patch)\n",
    "    # reduce to 2D piano roll (pitch vs. time)\n",
    "    curr_pred = curr_pred[0, :, :, 0]\n",
    "    all_pred.append(curr_pred)\n",
    "    \n",
    "    offset += patch_len\n",
    "    \n",
    "    if offset > n_frames-patch_len:\n",
    "        break\n",
    "        \n",
    "# finally, let's combine all patch predictions into one piano roll\n",
    "all_pred = np.hstack(all_pred)\n",
    "\n",
    "print(f\"Final shape: {all_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae18d5f",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0db2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_targets_test.shape)\n",
    "\n",
    "pl.figure(figsize=(15,6))\n",
    "pl.subplot(2,2,1)\n",
    "librosa.display.specshow(all_cqts_test, sr=44100, x_axis='time', y_axis='cqt_note', ax=pl.gca())\n",
    "pl.title('CQT')\n",
    "pl.subplot(2,2,2)\n",
    "pl.imshow(all_pred, aspect=\"auto\", origin=\"lower\",cmap=\"Greys\")\n",
    "pl.title('Predicted Pitch Contour')\n",
    "pl.subplot(2,2,4)\n",
    "pl.imshow(all_targets_test, aspect=\"auto\", origin=\"lower\",cmap=\"Greys\")\n",
    "pl.title('True Pitch Contour')\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_wav = os.path.join(dir_dataset, file_prefixes[3] + '.wav')\n",
    "x, fs = librosa.load(fn_wav)\n",
    "ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb73c8",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "That looks promising, still several overtones end up as f0 candidates but this usually is a matter of more training data.\n",
    "\n",
    "\n",
    "## Possible next steps\n",
    "\n",
    "- run a proper evaluation by comparing the predicted piano roll with the ground truth using the **mir_eval** toolbox (https://craffel.github.io/mir_eval/#module-mir_eval.multipitch)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de3bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd95b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
