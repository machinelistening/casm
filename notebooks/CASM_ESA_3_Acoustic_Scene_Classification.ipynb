{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "## Computational Analysis of Sound and Music\n",
    "\n",
    "# ESA 3 - Acoustic Scene Classification\n",
    "\n",
    "Dr.-Ing. Jakob Abe√üer, jakob.abesser@idmt.fraunhofer.de\n",
    "\n",
    "**Last update:** 29.05.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017d747",
   "metadata": {},
   "source": [
    "**TODO** \n",
    "- spectrogram-based data augmentation (mixup, spec masking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded3d3a",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "In this notebook, we use a small dataset of **acoustic scene recordings**.\n",
    "We will study how to \n",
    "- implement a convolutional recurrent neural network (CRNN) in comparison with our previous CNN\n",
    "- implement the **mixup** and **spec masking** data augmentation in a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1612f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import os\n",
    "import matplotlib\n",
    "import librosa\n",
    "import matplotlib.pyplot as pl\n",
    "import platform\n",
    "import IPython.display as ipd\n",
    "import wget\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b858785",
   "metadata": {},
   "source": [
    "## Dataset download & pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb4a66",
   "metadata": {},
   "source": [
    "The **asc_mini** dataset includes audio recordings from the following **10 acoustic scene classes**:\n",
    "- airport\n",
    "- bus\n",
    "- metro\n",
    "- metro station\n",
    "- part\n",
    "- public square\n",
    "- shopping mall\n",
    "- street pedestrian\n",
    "- street traffic\n",
    "- tram\n",
    "\n",
    "and **8 short 2.5s audio clips** from each ASC class.\n",
    "The original audio samples were taken from the **TAU Urban Acoustic Scenes 2020 3Class dataset**, which is described here: https://dcase.community/challenge2020/task-acoustic-scene-classification (under \"Development set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('asc_mini.zip'):\n",
    "    print('Please wait a couple of seconds ...')\n",
    "    wget.download('https://github.com/machinelistening/machinelistening.github.io/blob/master/asc_mini.zip?raw=true', \n",
    "                      out='asc_mini.zip', bar=None)\n",
    "    print('asc_mini.zip downloaded successfully ...')\n",
    "else:\n",
    "    print('Files already exist!')\n",
    "    \n",
    "if not os.path.isdir('asc_mini'):\n",
    "    print(\"Let's unzip the file ... \")\n",
    "    assert os.path.isfile('asc_mini.zip')\n",
    "    with zipfile.ZipFile('asc_mini.zip', 'r') as f:\n",
    "        f.extractall('.')\n",
    "    assert os.path.isdir('asc_mini')\n",
    "    print(\"All done :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample rate\n",
    "fs = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36751d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the subdirectories (which provide us the animal classes)\n",
    "dir_dataset = 'asc_mini'\n",
    "fn_wav_list = glob.glob(os.path.join(dir_dataset, '*.wav'))\n",
    "\n",
    "class_label = []\n",
    "file_num_in_class = []\n",
    "for fn_wav in fn_wav_list:\n",
    "    fn_wav_base = os.path.basename(fn_wav)\n",
    "    parts = fn_wav_base.split('_')\n",
    "    class_label.append(parts[0])\n",
    "    file_num_in_class.append(int(parts[-1].replace('.wav', ''))-1)\n",
    "\n",
    "for i in range(len(fn_wav_list)):\n",
    "    print(f\"File {i+1}: {os.path.basename(fn_wav_list[i])}, class = {class_label[i]}, number in class = {file_num_in_class[i]}\")\n",
    "n_files = len(fn_wav_list)\n",
    "    \n",
    "# this vector includes a \"counter\" for each file within its class, we use it later ...\n",
    "file_num_in_class = np.array(file_num_in_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d86391",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = sorted(list(set(class_label)))\n",
    "class_id = np.array([unique_classes.index(_) for _ in class_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1a86e",
   "metadata": {},
   "source": [
    "Let's listen to some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d84e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (0, 12, 24, 36, 48)\n",
    "for i in idx:\n",
    "    x, fs = librosa.load(fn_wav_list[i])\n",
    "    print(class_label[i])\n",
    "    ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534a702",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3535637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melspec(fn_wav, n_bins=128):\n",
    "    \"\"\" Compute Mel spectrogram with logarithmic magnitude scaling \n",
    "    Args:\n",
    "        fn_wav (str): WAV file name\n",
    "        n_bins (int): Number of Mel frequency bins\n",
    "    Returns:\n",
    "        mel_spec (2d np.ndarray): Mel spectrogram (n_bins x n_frames)\n",
    "    \"\"\"\n",
    "    x, fs = librosa.load(fn_wav, mono=True, sr=44100)\n",
    "    S = librosa.feature.melspectrogram(y=x, sr=fs, n_mels=n_bins, fmax=fs/2)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096caaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = []\n",
    "for fn_wav in fn_wav_list:\n",
    "    feat.append(compute_melspec(fn_wav))\n",
    "feat = np.array(feat)\n",
    "feat = np.expand_dims(feat, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e60440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature matrix shape: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7131ab",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa349a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = np.where(file_num_in_class < 5)[0] # 5 training files\n",
    "is_test = np.where(file_num_in_class >= 5)[0] # e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feat[is_train, :]\n",
    "X_test = feat[is_test, :]\n",
    "\n",
    "y_train = class_id[is_train]\n",
    "y_test = class_id[is_test]\n",
    "\n",
    "# one-hot-encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Data standardization\n",
    "X_train -= np.mean(X_train)\n",
    "X_train /= np.std(X_train)\n",
    "\n",
    "X_test -= np.mean(X_test)\n",
    "X_test /= np.std(X_test)\n",
    "\n",
    "print(f\"X_train shape {X_train.shape}\")\n",
    "print(f\"X_test shape {X_test.shape}\")\n",
    "\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b618e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8 \n",
    "n_epochs=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3361b62",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "We use the same CNN model as in the previous seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d16f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_output_dim):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    x = None\n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            x = inp\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation=\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([tf.keras.layers.GlobalAveragePooling2D()(x),\n",
    "                                     tf.keras.layers.GlobalMaxPooling2D()(x)])\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    out = tf.keras.layers.Dense(num_output_dim, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    " \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8aeb7",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "As a baseline experiment, we train our model on the clean training set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ad572",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:] \n",
    "model_s1 = create_cnn_model(input_shape, 10)\n",
    "model_s1.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbbff6",
   "metadata": {},
   "source": [
    "## Data Augmentation \n",
    "\n",
    "Let's implement the **Mixup** and **Spectrogram Masking** methods first by-hand to get a better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b7722",
   "metadata": {},
   "source": [
    "### (1) Spectrogram Masking\n",
    "\n",
    "We want to create blocks in the spectrogram that we will mask with zeros. We distinguish between\n",
    "- frequency blocks (they span the entire spectrogram duration but only a small frequency band)\n",
    "- time blocks (try span only a short time segment but the entire frequency range)\n",
    "\n",
    "For each spectrogram, we'll randomly mask 2 frequency blocks and 2 time blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad29d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_spectral_masking(orig_spec, n_time_blocks=2, n_freq_blocks=2, max_width=10, max_height=15):\n",
    "    spec=np.copy(orig_spec)\n",
    "    n_freq, n_time = spec.shape\n",
    "    \n",
    "    for i in range(n_time_blocks):\n",
    "        start_frame = int(np.floor(np.random.rand(1)*n_time))\n",
    "        duration = int(np.ceil(np.random.rand(1)*max_width))\n",
    "        spec[:, start_frame:start_frame+duration] = 0\n",
    "        \n",
    "    for i in range(n_freq_blocks):\n",
    "        low_bin = int(np.floor(np.random.rand(1)*n_freq))\n",
    "        range_ = int(np.ceil(np.random.rand(1)*max_height))\n",
    "        spec[low_bin:low_bin+range_, :] = 0\n",
    "        \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4491b8d",
   "metadata": {},
   "source": [
    "Let's try this for one of the spectrograms and create three randomly augmented versions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "pl.figure()\n",
    "pl.subplot(2,2,1)\n",
    "pl.imshow(feat[0, :, :, 0], origin=\"lower\", aspect=\"auto\")\n",
    "for i in range(3):\n",
    "    pl.subplot(2,2,2+i)\n",
    "    pl.imshow(augment_spectral_masking(feat[0, :, :, 0]), origin=\"lower\", aspect=\"auto\")\n",
    "pl.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9e8c7",
   "metadata": {},
   "source": [
    "### (2) Mixup\n",
    "\n",
    "**Mixup** was introduced in \n",
    "  - Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. \"mixup: Beyond empirical risk minimization.\"  arXiv preprint arXiv:1710.09412 (2017).\n",
    "  \n",
    "Mixup is a data augmentation technique where training samples are created by combining pairs of inputs and their corresponding labels in a weighted manner. \n",
    "\n",
    "Mixup is controlled by only one parameter: $\\alpha$. It controls the **strength of interpolation between pairs of examples and their labels**. \n",
    "It determines the **shape of the Beta distribution** used to sample the mixing coefficients, where a higher alpha leads to more mixing between the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11919956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_mixup(spec1, spec2, target1, target2, alpha):\n",
    "    \n",
    "    # sample mixing coefficient from beta distribution\n",
    "    t = np.random.beta(alpha, alpha)\n",
    "\n",
    "    # Apply MixUp\n",
    "    spec = t*spec1 + (1-t)*spec2\n",
    "    target = t*target1 + (1-t)*target2\n",
    "    \n",
    "    return spec, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ad37c",
   "metadata": {},
   "source": [
    "Again, let's try this with two random spectrograms from our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab044811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "idx1 = 2\n",
    "idx2 = 20\n",
    "\n",
    "pl.figure()\n",
    "pl.subplot(2,2,1)\n",
    "pl.imshow(X_train[idx1, :, :, 0], origin=\"lower\", aspect=\"auto\")\n",
    "pl.title(f\"Orig spec 1\", fontsize=8)\n",
    "pl.axis(\"off\")\n",
    "print(f\"Target 1 {y_train[idx1, :]}\")\n",
    "    \n",
    "pl.subplot(2,2,2)\n",
    "pl.imshow(X_train[idx2, :, :, 0], origin=\"lower\", aspect=\"auto\")\n",
    "pl.title(f\"Orig spec 2\", fontsize=8)\n",
    "pl.axis(\"off\")\n",
    "print(f\"Target 2 {y_train[idx2, :]}\")\n",
    "\n",
    "# let's create two mixed versions thereof\n",
    "for i in range(2):\n",
    "    X, y = augment_mixup(X_train[idx1, :, :, 0], \n",
    "                         X_train[idx2, :, :, 0], \n",
    "                         y_train[idx1, :],\n",
    "                         y_train[idx2, :],\n",
    "                         0.7)\n",
    "    pl.subplot(2,2,3+i)\n",
    "    pl.imshow(X, origin=\"lower\", aspect=\"auto\")\n",
    "    pl.title(f\"Mixup result\", fontsize=8)\n",
    "    print(f\"Target (mix) {y}\")\n",
    "    pl.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64850097",
   "metadata": {},
   "source": [
    "## Apply data augmentation to enhance training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2f836",
   "metadata": {},
   "source": [
    "### (1) Spectral Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = np.copy(X_train)\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_new[i, :, :, 0] = augment_spectral_masking(X_train[i, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_new.shape[1:] \n",
    "model_s2 = create_cnn_model(input_shape, 10)\n",
    "model_s2.fit(X_train_new, y_train, batch_size=batch_size, epochs=n_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f115ec",
   "metadata": {},
   "source": [
    "### (3) Mixup\n",
    "\n",
    "Here, we random pairs within our training dataset such that we have the same number of training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4474681",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = np.copy(X_train)\n",
    "y_train_new = np.copy(y_train)\n",
    "n_train_examples = X_train.shape[0]\n",
    "all_idx = np.arange(n_train_examples)\n",
    "for i in range(n_train_examples):\n",
    "    np.random.shuffle(all_idx)\n",
    "    X_train_new[i, :, :, 0], \\\n",
    "    y_train_new[i, :] = augment_mixup(X_train[all_idx[0], :, :, 0], \n",
    "                                      X_train[all_idx[1], :, :, 0], \n",
    "                                      y_train[all_idx[0], :], \n",
    "                                      y_train[all_idx[1], :], \n",
    "                                      0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab98f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_new.shape[1:] \n",
    "model_s3 = create_cnn_model(input_shape, 10)\n",
    "model_s3.fit(X_train_new, y_train_new, batch_size=batch_size, epochs=n_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3b47e",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate both models on the test set\n",
    "acc = np.zeros(3)\n",
    "for i, model in enumerate((model_s1, model_s2, model_s3)):\n",
    "\n",
    "    # evaluate model on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    class_id_test = np.argmax(y_test, axis=1)\n",
    "    class_id_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "    acc[i] = accuracy_score(class_id_test, class_id_test_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure()\n",
    "pl.plot(acc, 'o-')\n",
    "pl.xticks((0, 1, 2), ('no aug', 'spectral masking', 'mixup'))\n",
    "pl.ylabel('Accuracy')\n",
    "pl.xlabel('Strategy')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c0c5b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- combine spectral masking and mixup\n",
    "- integrate them into a data generator to have random data augmentation results in each epoch\n",
    "- create **more** augmented results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22046794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
