{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c49dcbd",
   "metadata": {},
   "source": [
    "# Computational Analysis of Sound and Music\n",
    "\n",
    "# A3 - Sound Perception\n",
    "\n",
    "Dr.-Ing. Jakob Abe√üer, jakob.abesser@idmt.fraunhofer.de\n",
    "\n",
    "**Last update:** 29.03.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f84403",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "In this notebook, you will learn \n",
    " - how to compute a chromagram feature\n",
    " - how chord inversions look in a chromagram representation\n",
    " - how to compute MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wget\n",
    "import os\n",
    "import matplotlib\n",
    "import librosa\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as pl\n",
    "import platform\n",
    "import IPython.display as ipd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6898af",
   "metadata": {},
   "source": [
    "Load audio files if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('piano.wav') or not os.path.isfile('c_maj_inv_2_sounds.wav'):\n",
    "    for fn in ('piano.wav', 'c_maj_inv_2_sounds.wav'):\n",
    "        wget.download('https://github.com/machinelistening/machinelistening.github.io/blob/master/{}?raw=true'.format(fn), \n",
    "                      out=fn, bar=None)\n",
    "else:\n",
    "    print('Files already exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974366a",
   "metadata": {},
   "source": [
    "### Reminder: Constant-Q Transform (CQT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8409e6",
   "metadata": {},
   "source": [
    "Reminder: Here's the CQT of our piano example from the last seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_wav = 'piano.wav'\n",
    "x, fs = librosa.load(fn_wav)\n",
    "ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c45900",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_octaves = 5  # let's capture 5 octaves starting from C1\n",
    "bins_per_octave = 12  # let's choose a frequency resolution of 100 cent (= one frequency bin per semitone)\n",
    "C = np.abs(librosa.cqt(x, sr=fs, n_bins=n_octaves*bins_per_octave , bins_per_octave=bins_per_octave))\n",
    "C = librosa.amplitude_to_db(C)  # dB magnitude scaling\n",
    "fig, ax = pl.subplots()\n",
    "img = librosa.display.specshow(C, sr=fs, x_axis='time', y_axis='cqt_note', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0d691",
   "metadata": {},
   "source": [
    "### CENS (Chroma Energy Normalize) Chroma Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a20580",
   "metadata": {},
   "source": [
    "Let's first check the librosa documentation: https://librosa.org/doc/main/generated/librosa.feature.chroma_cens.html\n",
    "\n",
    "The most important parameters are \n",
    "  - **y** - audio sample vector ($x$)\n",
    "  - **sr** - sampling rate of the audio signal (in Hz)\n",
    "  - **hop_length** - see above (STFT)\n",
    "  - **f_min** - minimum frequency (we can use the default value of 32.70 Hz which corresponds to the note C1)\n",
    "  - **n_bins** - total number of frequency bins (e.g., for a frequency resolution of one bin per semitone and 4 octaves, this would be 4 * 12 = 48)\n",
    "  - **bins_per_octave** - Logarithmic frequency resolution (frequency bins per octave, commonly: 12 or 36)\n",
    "  - **tuning** - Tuning offset (can be used if known tuning frequency of an audio recording deviates from 440 Hz) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length=128\n",
    "cens = librosa.feature.chroma_cens(y=x, sr=fs, hop_length=hop_length, fmin=None, tuning=None, n_chroma=12, n_octaves=7, bins_per_octave=36)\n",
    "t_max = len(x) / fs\n",
    "\n",
    "pl.figure()\n",
    "pl.imshow(cens, aspect=\"auto\", interpolation=\"None\", origin=\"lower\", extent=[0, t_max, 0, cens.shape[0]])\n",
    "# let's create an interpretable pitch axis\n",
    "pl.yticks([0.5, 2.5, 4.5, 5.5, 7.5, 9.5, 11.5], ['C', 'D', 'E', 'F', 'G', 'A', 'B'])\n",
    "pl.xlabel('Time (seconds)')\n",
    "pl.ylabel('Pitch Class')\n",
    "pl.title('CENS')\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee45ac",
   "metadata": {},
   "source": [
    "The chroma features give a nice visual summary of the pitch class distribution over time. \n",
    "\n",
    "**Note**: this is not a transcription as the pitch classes of the partial frequencies also affect this representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25646384",
   "metadata": {},
   "source": [
    "### Chord Inversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19e8af",
   "metadata": {},
   "source": [
    "Now we'll analyze a new file. It has 4 successive chords\n",
    "  - C major (root position): C E G\n",
    "  - C/E (first inversion): E G C\n",
    "  - C/G (second inversion): G C E\n",
    "  - C major (root position, one octave higher)\n",
    "  \n",
    "The chord sequence is first played using a piano, then repeated using a synthesizer sound.\n",
    "\n",
    "Here's the pianoroll view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://github.com/machinelistening/machinelistening.github.io/blob/master/c_maj_inv_2_sounds.png?raw=true\"\n",
    "ipd.display(ipd.Image(url=image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41c75d",
   "metadata": {},
   "source": [
    "Let's listen to the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_wav = 'c_maj_inv_2_sounds.wav'\n",
    "x, fs = librosa.load(fn_wav)\n",
    "ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec1f98",
   "metadata": {},
   "source": [
    "Now let's have a look to the CENS Chromagram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length=128\n",
    "cens = librosa.feature.chroma_cens(y=x, sr=fs, hop_length=hop_length, fmin=None, tuning=None, n_chroma=12, n_octaves=7, bins_per_octave=36)\n",
    "t_max = len(x) / fs\n",
    "\n",
    "pl.figure()\n",
    "pl.imshow(cens, aspect=\"auto\", interpolation=\"None\", origin=\"lower\", extent=[0, t_max, 0, cens.shape[0]])\n",
    "# let's create an interpretable pitch axis\n",
    "pl.yticks([0.5, 2.5, 4.5, 5.5, 7.5, 9.5, 11.5], ['C', 'D', 'E', 'F', 'G', 'A', 'B'])\n",
    "pl.xlabel('Time (seconds)')\n",
    "pl.ylabel('Pitch Class')\n",
    "pl.title('CENS')\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a0fc6",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "1) Why is the chromagram pattern not changing for different inversions?\n",
    "2) Why is the chromagram pattern not changing for different instrument timbres?\n",
    "3) Can you guess where the \"transient\" (vertical) structures come frome?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ce914",
   "metadata": {},
   "source": [
    "For comparison, let's check the CQT again, here we should see how the fundamental frequency and the harmonics are increasing in frequency along the chord sequence (similar to the piano roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_octaves = 7  # let's capture 7 octaves starting from C1\n",
    "bins_per_octave = 12  # let's choose a frequency resolution of 100 cent (= one frequency bin per semitone)\n",
    "C = np.abs(librosa.cqt(x, sr=fs, n_bins=n_octaves*bins_per_octave , bins_per_octave=bins_per_octave))\n",
    "C = librosa.amplitude_to_db(C)  # dB magnitude scaling\n",
    "fig, ax = pl.subplots()\n",
    "img = librosa.display.specshow(C, sr=fs, x_axis='time', y_axis='cqt_note', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4242b",
   "metadata": {},
   "source": [
    "### Mel-Frequency Cepstral Coefficients (MFCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc949e63",
   "metadata": {},
   "source": [
    "Let's first check the librosa documentation: https://librosa.org/doc/main/generated/librosa.cqt.html\n",
    "\n",
    "The most important parameters are \n",
    "  - **y** - audio sample vector ($x$)\n",
    "  - **sr** - sampling rate of the audio signal (in Hz)\n",
    "  - **n_fft** - see above (STFT)\n",
    "  - **hop_length** - see above (STFT)\n",
    "  - **n_mfcc** - Number of MFC coefficients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute and visualize 13 MFCCs\n",
    "n_mfcc = 20\n",
    "n_fft = 2048\n",
    "hop_length = 1024\n",
    "mf = librosa.feature.mfcc(y=x, sr=fs, S=None, n_mfcc=n_mfcc, dct_type=2, norm='ortho', lifter=0, n_fft=n_fft, hop_length=hop_length)\n",
    "pl.figure()\n",
    "pl.imshow(mf, aspect=\"auto\", origin=\"lower\", extent=[0, t_max, 0, mf.shape[0]])\n",
    "pl.xlabel('Time [s]')\n",
    "pl.ylabel('MFCC')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1e720",
   "metadata": {},
   "source": [
    "We will use the MFCC features in the upcoming seminars as timbre feature for different classification tasks ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471a664",
   "metadata": {},
   "source": [
    "Done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff80250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
